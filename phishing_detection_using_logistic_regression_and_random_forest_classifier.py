# -*- coding: utf-8 -*-
"""Phishing Detection Using Logistic Regression and Random Forest Classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hd_9cJo7J9eJLyuc-xHKWXMkfJEMLEDW

##Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import mutual_info_classif
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier as cuRfc
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report,
)

pd.set_option('display.max_columns', None)
plt.rcParams['figure.figsize'] = (12,6)

"""## Predicting Phishing Web Page Using Machine Learning

Phishing is a method of trying to gather personal information using deceptive e-mails and websites.

In this notebook, we will read the data and describe the features that highlight phishing
websites before moving to model prediction

## Loading the dataset
"""

data = pd.read_csv(
    "https://raw.githubusercontent.com/khuynh22/Phishing-Detection/"
    "main/Phishing_Legitimate_full.csv"
)

"""## Converting dataset

In this phase we convert float64 and int64 columns to 32-bit types. This saves
memory and prepares the data for using sklearn random forest models.

As we can see, the data has 10k rows and 50 columns including labels
"""

float_cols = data.select_dtypes('float64').columns
for c in float_cols:
    data[c] = data[c].astype('float32')

int_cols = data.select_dtypes('int64').columns
for c in int_cols:
    data[c] = data[c].astype('int32')

data.info()

data.rename(columns = {'CLASS_LABEL': 'labels'}, inplace = True)

"""## Viewing the data"""

data.sample(5)

"""## Summarizing Statistics

By using the describe method, we can see some columns have high variance while others
have smaller variance. This is due to larger values and wider ranges in some columns.
"""

data.describe()

"""## Balanced/Imbalanced Checking"""

data['labels'].value_counts().plot(kind = 'bar')

"""## Spearman Correlation

By looking at the Spearman correlation, we can find which features are linearly
correlated with phishing labels.
"""

def corr_heatmap(data, idx_s, idx_e):
  y = data['labels']
  temp = data.iloc[:, idx_s:idx_e]
  if 'id' in temp.columns:
    del temp['id']
  temp["labels"] = y
  sns.heatmap(temp.corr(), annot= True, fmt = '.2f')
  plt.show()

"""## Heatmap of first 50 columns

By looking at the first 10 columns against labels, we can conclude that none of the
features have a strong correlation with the labels. However, NumDash has some
negative effect, which may mean that fewer dashes imply a higher probability of
phishing.
"""

# First 10 columns
corr_heatmap(data, 0, 10)

"""## Columns 10 to 20

There are no strong or even medium level strength correlation features with labels
"""

# Column 11 to 20
corr_heatmap(data, 10, 20)

"""##Columns 20 to 30

Columns 20 to 30
Still no strong correlation feature
"""

# Column 21 to 30
corr_heatmap(data, 20, 30)

"""## Columns 30 to 40

A few features show linear correlations with the dependent variable:

* InsecureForms increases phishing probability at higher values.
* PctNullSelfRedirectHyperlinks shares that positive correlation.
* FequentDomainNameMismatch has medium positive correlation.
* SubmitInfoToEmail suggests that sites asking users to email details are more likely phishing.
"""

# Column 31 to 40
corr_heatmap(data, 30, 40)

"""## Columns 40 to 50

PctExtNullSelfRedirectHyperlinksRT has a negative correlation with labels, which may
mean that more null self-redirect hyperlinks increase the probability of phishing.
"""

# Column 41 to 50
corr_heatmap(data, 40, 50)

"""##Mutual Info Classifier

We will use mutual info to find both linear and nonlinear correlations between the features
and the labels.
"""

X = data.drop(['id', 'labels'], axis = 1)
y = data['labels']

discrete_features = X.dtypes == int

"""Here we process the scores and notice that mutual info now produces a list that differs
from the Spearman correlation."""

# Process the scores and compare with spearman corr
mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)
mi_scores = pd.Series(mi_scores, name = 'MI Scores', index = X.columns)
mi_scores = mi_scores.sort_values(ascending = False)
mi_scores

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("MI Scores")

plt.figure(dpi=100, figsize=(12,12))
plot_mi_scores(mi_scores)

"""##Prediction
We start with logistic regression as a baseline before training a random forest classifier.
Evaluation metrics include accuracy, precision, recall, and f1 score.
"""

"""##Train logistic models
This method repeats logistic regression training with the top N features. It keeps
hyperparameters fixed so we can identify the best feature count purely by evaluation
metrics.
"""

def train_logistic(data, top_n):
    top_n_features = mi_scores.sort_values(ascending=False).head(top_n).index.tolist()
    X = data[top_n_features]
    y = data['labels']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

    lr = LogisticRegression(max_iter=10000)
    lr.fit(X_train, y_train)

    y_pred = lr.predict(X_test)

    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)

    return precision, recall, f1, accuracy

"""The loop trains logistic models on the top N features from 20 to 50 to find the best count."""

arr = []
for i in range(20, 51):
    precision, recall, f1, accuracy = train_logistic(data, i)
    print(
        (
            "Performance for Logistic Model (Top {}) precision: {}, recall: {}, "
            "f1 score: {}, accuracy: {}"
        ).format(
            i,
            precision,
            recall,
            f1,
            accuracy,
        )
    )
    arr.append([i, precision, recall, f1, accuracy])

df = pd.DataFrame(arr, columns=['num_of_features', 'precision', 'recall', 'f1_score', 'accuracy'])
df

"""## Visualize Logistic Regression Performance

Metrics fluctuate as more features are added. We aim to maximize all metrics, so we
target the region where accuracy, precision, recall, and f1 stay strong, which starts
to happen near 39 features.
"""

sns.lineplot(x = 'num_of_features', y = 'precision', data = df, label = 'Precision Score')
sns.lineplot(x = 'num_of_features', y = 'recall', data = df, label = 'Recall Score')
sns.lineplot(x = 'num_of_features', y = 'f1_score', data = df, label = 'F1 Score')
sns.lineplot(x = 'num_of_features', y = 'accuracy', data = df, label = 'Accuracy Score')

"""##Training Random Forest Classifier on GPU

We apply the same pipeline used for logistic regression but swap in a random
forest classifier to beat the baseline.
"""

def train_rfc(data, top_n):
    top_n_features = mi_scores.sort_values(ascending=False).head(top_n).index.tolist()
    X = data[top_n_features]
    y = data['labels']

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

    rfc = cuRfc(n_estimators=500,
                criterion="gini",
                max_depth=32,
                max_features=1.0,
                n_jobs=128)

    rfc.fit(X_train, y_train)

    y_pred = rfc.predict(X_test)

    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)

    return precision, recall, f1, accuracy

arr = []
for i in range(20, 51):
    precision, recall, f1, accuracy = train_rfc(data, i)
    print(
        (
            "Performance for RFC Model (Top {}) precision: {}, recall: {}, "
            "f1 score: {}, accuracy: {}"
        ).format(
            i,
            precision,
            recall,
            f1,
            accuracy,
        )
    )
    arr.append([i, precision, recall, f1, accuracy])

df = pd.DataFrame(arr, columns=['num_of_features', 'precision', 'recall', 'f1_score', 'accuracy'])
df.head()

"""## Visualize Random Forest Performance

Our goal is to beat the logistic regression baseline; visualizing the metrics shows
that 32 features deliver the best overall performance, one fewer than the logistic
setting.
"""

sns.lineplot(x='num_of_features', y='precision', data=df, label='Precision Score')
sns.lineplot(x='num_of_features', y='recall', data=df, label='Recall Score')
sns.lineplot(x='num_of_features', y='f1_score', data=df, label='F1 Score')
sns.lineplot(x='num_of_features', y='accuracy', data=df, label='Acc Score')

"""##Final Random Forest Mode

Lets train the final random forest model based on the optimal N number of features
"""

top_n_features = mi_scores.sort_values(ascending=False).head(32).index.tolist()
X = data[top_n_features]
y = data['labels']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)

rfc = cuRfc(n_estimators=500,
            criterion="gini",
            max_depth=32,
            max_features=1.0,
            n_jobs=128)

rfc.fit(X_train, y_train)

y_pred = rfc.predict(X_test)

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

print(
    (
        "Performance for RFC Model (Top {}) precision: {}, recall: {}, "
        "f1 score: {}, accuracy: {}"
    ).format(
        27,
        precision,
        recall,
        f1,
        accuracy,
    )
)

"""## Performance

The final RFC model predicts phishing with roughly 98% accuracy, precision, and recall,
which shows high confidence in distinguishing phishing and legitimate sites.
"""

print(classification_report(y_test, y_pred))
